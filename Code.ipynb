{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Security Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5-JNsTojVqu",
        "outputId": "82eaf16c-51bc-43f5-c85d-24b1905d4404"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhKBQTVbjh6q"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # for plotting beautiful graphs\n",
        "\n",
        "# train test split from sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import Torch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "# from torch.utils.data import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "# example of ordinal encoding for a neural network\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbKOwuHkjif9"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('/content/drive/MyDrive/Samsung Laptop/AI Cybersecurity Def/Project/MNIST_dataset/train.csv', dtype=np.float32)\n",
        "test = pd.read_csv('/content/drive/MyDrive/Samsung Laptop/AI Cybersecurity Def/Project/MNIST_dataset/test.csv', dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "Dt2MQBoqFnUO",
        "outputId": "3599d632-cda0-4497-ab2e-286762e649e0"
      },
      "source": [
        "#https://www.kaggle.com/fedesoriano/stroke-prediction-dataset\n",
        "PATH_2_DATA = \"/content/drive/MyDrive/Samsung Laptop/AI Cybersecurity Def/Project/Stroke_dataset/healthcare-dataset-stroke-data.csv\"\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Samsung Laptop/AI Cybersecurity Def/Project/Stroke_dataset/healthcare-dataset-stroke-data.csv\")\n",
        "data = data.dropna()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>ever_married</th>\n",
              "      <th>work_type</th>\n",
              "      <th>Residence_type</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>smoking_status</th>\n",
              "      <th>stroke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9046</td>\n",
              "      <td>Male</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>228.69</td>\n",
              "      <td>36.6</td>\n",
              "      <td>formerly smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31112</td>\n",
              "      <td>Male</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Rural</td>\n",
              "      <td>105.92</td>\n",
              "      <td>32.5</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60182</td>\n",
              "      <td>Female</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>171.23</td>\n",
              "      <td>34.4</td>\n",
              "      <td>smokes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1665</td>\n",
              "      <td>Female</td>\n",
              "      <td>79.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Rural</td>\n",
              "      <td>174.12</td>\n",
              "      <td>24.0</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>56669</td>\n",
              "      <td>Male</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>186.21</td>\n",
              "      <td>29.0</td>\n",
              "      <td>formerly smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5104</th>\n",
              "      <td>14180</td>\n",
              "      <td>Female</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>children</td>\n",
              "      <td>Rural</td>\n",
              "      <td>103.08</td>\n",
              "      <td>18.6</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5106</th>\n",
              "      <td>44873</td>\n",
              "      <td>Female</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Urban</td>\n",
              "      <td>125.20</td>\n",
              "      <td>40.0</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5107</th>\n",
              "      <td>19723</td>\n",
              "      <td>Female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Rural</td>\n",
              "      <td>82.99</td>\n",
              "      <td>30.6</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5108</th>\n",
              "      <td>37544</td>\n",
              "      <td>Male</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Rural</td>\n",
              "      <td>166.29</td>\n",
              "      <td>25.6</td>\n",
              "      <td>formerly smoked</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5109</th>\n",
              "      <td>44679</td>\n",
              "      <td>Female</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Govt_job</td>\n",
              "      <td>Urban</td>\n",
              "      <td>85.28</td>\n",
              "      <td>26.2</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4909 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  gender   age  ...   bmi   smoking_status stroke\n",
              "0      9046    Male  67.0  ...  36.6  formerly smoked      1\n",
              "2     31112    Male  80.0  ...  32.5     never smoked      1\n",
              "3     60182  Female  49.0  ...  34.4           smokes      1\n",
              "4      1665  Female  79.0  ...  24.0     never smoked      1\n",
              "5     56669    Male  81.0  ...  29.0  formerly smoked      1\n",
              "...     ...     ...   ...  ...   ...              ...    ...\n",
              "5104  14180  Female  13.0  ...  18.6          Unknown      0\n",
              "5106  44873  Female  81.0  ...  40.0     never smoked      0\n",
              "5107  19723  Female  35.0  ...  30.6     never smoked      0\n",
              "5108  37544    Male  51.0  ...  25.6  formerly smoked      0\n",
              "5109  44679  Female  44.0  ...  26.2          Unknown      0\n",
              "\n",
              "[4909 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "Ii-n64y7RdmD",
        "outputId": "346a8482-415a-40a2-9aa9-809f229243ec"
      },
      "source": [
        "df = data.iloc[:,:-1]\n",
        "column_names_for_onehot = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
        "X = pd.get_dummies(df, columns=column_names_for_onehot, drop_first=True)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender_Male</th>\n",
              "      <th>gender_Other</th>\n",
              "      <th>ever_married_Yes</th>\n",
              "      <th>work_type_Never_worked</th>\n",
              "      <th>work_type_Private</th>\n",
              "      <th>work_type_Self-employed</th>\n",
              "      <th>work_type_children</th>\n",
              "      <th>Residence_type_Urban</th>\n",
              "      <th>smoking_status_formerly smoked</th>\n",
              "      <th>smoking_status_never smoked</th>\n",
              "      <th>smoking_status_smokes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9046</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>228.69</td>\n",
              "      <td>36.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31112</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>105.92</td>\n",
              "      <td>32.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60182</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>171.23</td>\n",
              "      <td>34.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1665</td>\n",
              "      <td>79.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>174.12</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>56669</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>186.21</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5104</th>\n",
              "      <td>14180</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>103.08</td>\n",
              "      <td>18.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5106</th>\n",
              "      <td>44873</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125.20</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5107</th>\n",
              "      <td>19723</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>82.99</td>\n",
              "      <td>30.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5108</th>\n",
              "      <td>37544</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>166.29</td>\n",
              "      <td>25.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5109</th>\n",
              "      <td>44679</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>85.28</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4909 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id   age  ...  smoking_status_never smoked  smoking_status_smokes\n",
              "0      9046  67.0  ...                            0                      0\n",
              "2     31112  80.0  ...                            1                      0\n",
              "3     60182  49.0  ...                            0                      1\n",
              "4      1665  79.0  ...                            1                      0\n",
              "5     56669  81.0  ...                            0                      0\n",
              "...     ...   ...  ...                          ...                    ...\n",
              "5104  14180  13.0  ...                            0                      0\n",
              "5106  44873  81.0  ...                            1                      0\n",
              "5107  19723  35.0  ...                            1                      0\n",
              "5108  37544  51.0  ...                            0                      0\n",
              "5109  44679  44.0  ...                            0                      0\n",
              "\n",
              "[4909 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "xYWqo1KvUfZ6",
        "outputId": "5e093d65-c10f-4555-c819-b709da29644b"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "# Create the Scaler object\n",
        "scaler = preprocessing.StandardScaler()\n",
        "# Fit your data on the scaler object\n",
        "scaled_df = scaler.fit_transform(X)\n",
        "X = pd.DataFrame(scaled_df, columns=X.columns)\n",
        "X = X.dropna()\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender_Male</th>\n",
              "      <th>gender_Other</th>\n",
              "      <th>ever_married_Yes</th>\n",
              "      <th>work_type_Never_worked</th>\n",
              "      <th>work_type_Private</th>\n",
              "      <th>work_type_Self-employed</th>\n",
              "      <th>work_type_children</th>\n",
              "      <th>Residence_type_Urban</th>\n",
              "      <th>smoking_status_formerly smoked</th>\n",
              "      <th>smoking_status_never smoked</th>\n",
              "      <th>smoking_status_smokes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.298312</td>\n",
              "      <td>1.051434</td>\n",
              "      <td>-0.328602</td>\n",
              "      <td>4.185032</td>\n",
              "      <td>2.706375</td>\n",
              "      <td>0.981345</td>\n",
              "      <td>1.189990</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>0.723884</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>0.864297</td>\n",
              "      <td>-0.436881</td>\n",
              "      <td>-0.394112</td>\n",
              "      <td>0.984080</td>\n",
              "      <td>2.184951</td>\n",
              "      <td>-0.766774</td>\n",
              "      <td>-0.427313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.255478</td>\n",
              "      <td>1.626390</td>\n",
              "      <td>-0.328602</td>\n",
              "      <td>4.185032</td>\n",
              "      <td>-0.005028</td>\n",
              "      <td>0.459269</td>\n",
              "      <td>1.189990</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>0.723884</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>0.864297</td>\n",
              "      <td>-0.436881</td>\n",
              "      <td>-0.394112</td>\n",
              "      <td>-1.016178</td>\n",
              "      <td>-0.457676</td>\n",
              "      <td>1.304165</td>\n",
              "      <td>-0.427313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.118363</td>\n",
              "      <td>0.255342</td>\n",
              "      <td>-0.328602</td>\n",
              "      <td>-0.238947</td>\n",
              "      <td>1.437358</td>\n",
              "      <td>0.701207</td>\n",
              "      <td>-0.840343</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>0.723884</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>0.864297</td>\n",
              "      <td>-0.436881</td>\n",
              "      <td>-0.394112</td>\n",
              "      <td>0.984080</td>\n",
              "      <td>-0.457676</td>\n",
              "      <td>-0.766774</td>\n",
              "      <td>2.340204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.647136</td>\n",
              "      <td>1.582163</td>\n",
              "      <td>3.043196</td>\n",
              "      <td>-0.238947</td>\n",
              "      <td>1.501184</td>\n",
              "      <td>-0.623083</td>\n",
              "      <td>-0.840343</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>0.723884</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>-1.157010</td>\n",
              "      <td>2.288955</td>\n",
              "      <td>-0.394112</td>\n",
              "      <td>-1.016178</td>\n",
              "      <td>-0.457676</td>\n",
              "      <td>1.304165</td>\n",
              "      <td>-0.427313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.952339</td>\n",
              "      <td>1.670617</td>\n",
              "      <td>-0.328602</td>\n",
              "      <td>-0.238947</td>\n",
              "      <td>1.768195</td>\n",
              "      <td>0.013595</td>\n",
              "      <td>1.189990</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>0.723884</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>0.864297</td>\n",
              "      <td>-0.436881</td>\n",
              "      <td>-0.394112</td>\n",
              "      <td>0.984080</td>\n",
              "      <td>2.184951</td>\n",
              "      <td>-0.766774</td>\n",
              "      <td>-0.427313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5104</th>\n",
              "      <td>-1.055680</td>\n",
              "      <td>-1.336844</td>\n",
              "      <td>-0.328602</td>\n",
              "      <td>-0.238947</td>\n",
              "      <td>-0.067750</td>\n",
              "      <td>-1.310695</td>\n",
              "      <td>-0.840343</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>-1.381436</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>-1.157010</td>\n",
              "      <td>-0.436881</td>\n",
              "      <td>2.537348</td>\n",
              "      <td>-1.016178</td>\n",
              "      <td>-0.457676</td>\n",
              "      <td>-0.766774</td>\n",
              "      <td>-0.427313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5106</th>\n",
              "      <td>0.394863</td>\n",
              "      <td>1.670617</td>\n",
              "      <td>-0.328602</td>\n",
              "      <td>-0.238947</td>\n",
              "      <td>0.420775</td>\n",
              "      <td>1.414286</td>\n",
              "      <td>-0.840343</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>0.723884</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>-1.157010</td>\n",
              "      <td>2.288955</td>\n",
              "      <td>-0.394112</td>\n",
              "      <td>0.984080</td>\n",
              "      <td>-0.457676</td>\n",
              "      <td>1.304165</td>\n",
              "      <td>-0.427313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5107</th>\n",
              "      <td>-0.793720</td>\n",
              "      <td>-0.363842</td>\n",
              "      <td>-0.328602</td>\n",
              "      <td>-0.238947</td>\n",
              "      <td>-0.511443</td>\n",
              "      <td>0.217332</td>\n",
              "      <td>-0.840343</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>0.723884</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>-1.157010</td>\n",
              "      <td>2.288955</td>\n",
              "      <td>-0.394112</td>\n",
              "      <td>-1.016178</td>\n",
              "      <td>-0.457676</td>\n",
              "      <td>1.304165</td>\n",
              "      <td>-0.427313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5108</th>\n",
              "      <td>0.048497</td>\n",
              "      <td>0.343796</td>\n",
              "      <td>-0.328602</td>\n",
              "      <td>-0.238947</td>\n",
              "      <td>1.328257</td>\n",
              "      <td>-0.419346</td>\n",
              "      <td>1.189990</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>0.723884</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>0.864297</td>\n",
              "      <td>-0.436881</td>\n",
              "      <td>-0.394112</td>\n",
              "      <td>-1.016178</td>\n",
              "      <td>2.184951</td>\n",
              "      <td>-0.766774</td>\n",
              "      <td>-0.427313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5109</th>\n",
              "      <td>0.385695</td>\n",
              "      <td>0.034205</td>\n",
              "      <td>-0.328602</td>\n",
              "      <td>-0.238947</td>\n",
              "      <td>-0.460867</td>\n",
              "      <td>-0.342945</td>\n",
              "      <td>-0.840343</td>\n",
              "      <td>-0.01399</td>\n",
              "      <td>0.723884</td>\n",
              "      <td>-0.065756</td>\n",
              "      <td>-1.157010</td>\n",
              "      <td>-0.436881</td>\n",
              "      <td>-0.394112</td>\n",
              "      <td>0.984080</td>\n",
              "      <td>-0.457676</td>\n",
              "      <td>-0.766774</td>\n",
              "      <td>-0.427313</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4909 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id       age  ...  smoking_status_never smoked  smoking_status_smokes\n",
              "0    -1.298312  1.051434  ...                    -0.766774              -0.427313\n",
              "2    -0.255478  1.626390  ...                     1.304165              -0.427313\n",
              "3     1.118363  0.255342  ...                    -0.766774               2.340204\n",
              "4    -1.647136  1.582163  ...                     1.304165              -0.427313\n",
              "5     0.952339  1.670617  ...                    -0.766774              -0.427313\n",
              "...        ...       ...  ...                          ...                    ...\n",
              "5104 -1.055680 -1.336844  ...                    -0.766774              -0.427313\n",
              "5106  0.394863  1.670617  ...                     1.304165              -0.427313\n",
              "5107 -0.793720 -0.363842  ...                     1.304165              -0.427313\n",
              "5108  0.048497  0.343796  ...                    -0.766774              -0.427313\n",
              "5109  0.385695  0.034205  ...                    -0.766774              -0.427313\n",
              "\n",
              "[4909 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3OkGSYUTxN_",
        "outputId": "16350554-5655-4c3d-f96a-012fc6d57740"
      },
      "source": [
        "y = data.iloc[:,-1].values\n",
        "#y = y.reshape((len(y), 1))\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "dIzcgtllTIRh",
        "outputId": "7c5e9690-210b-4551-a93e-ca2b033502f6"
      },
      "source": [
        "data_out = pd.concat([df_2, data.iloc[:,-1]], axis=1)\n",
        "data_out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender_Male</th>\n",
              "      <th>gender_Other</th>\n",
              "      <th>ever_married_Yes</th>\n",
              "      <th>work_type_Never_worked</th>\n",
              "      <th>work_type_Private</th>\n",
              "      <th>work_type_Self-employed</th>\n",
              "      <th>work_type_children</th>\n",
              "      <th>Residence_type_Urban</th>\n",
              "      <th>smoking_status_formerly smoked</th>\n",
              "      <th>smoking_status_never smoked</th>\n",
              "      <th>smoking_status_smokes</th>\n",
              "      <th>stroke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9046</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>228.69</td>\n",
              "      <td>36.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51676</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>202.21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31112</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>105.92</td>\n",
              "      <td>32.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60182</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>171.23</td>\n",
              "      <td>34.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1665</td>\n",
              "      <td>79.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>174.12</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5105</th>\n",
              "      <td>18234</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>83.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5106</th>\n",
              "      <td>44873</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125.20</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5107</th>\n",
              "      <td>19723</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>82.99</td>\n",
              "      <td>30.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5108</th>\n",
              "      <td>37544</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>166.29</td>\n",
              "      <td>25.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5109</th>\n",
              "      <td>44679</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>85.28</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5110 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id   age  ...  smoking_status_smokes  stroke\n",
              "0      9046  67.0  ...                      0       1\n",
              "1     51676  61.0  ...                      0       1\n",
              "2     31112  80.0  ...                      0       1\n",
              "3     60182  49.0  ...                      1       1\n",
              "4      1665  79.0  ...                      0       1\n",
              "...     ...   ...  ...                    ...     ...\n",
              "5105  18234  80.0  ...                      0       0\n",
              "5106  44873  81.0  ...                      0       0\n",
              "5107  19723  35.0  ...                      0       0\n",
              "5108  37544  51.0  ...                      0       0\n",
              "5109  44679  44.0  ...                      0       0\n",
              "\n",
              "[5110 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mlB72aaHBMt",
        "outputId": "54abafec-e6c0-45bc-cc6d-76059b927f19"
      },
      "source": [
        "# load and summarize the dataset\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "\t# load the dataset as a pandas DataFrame\n",
        "\tdata = read_csv(filename, header=None)\n",
        "\t# retrieve numpy array\n",
        "\tdataset = data.values\n",
        "\t# split into input (X) and output (y) variables\n",
        "\tX = dataset[:, 1:-1]\n",
        "\ty = dataset[:,-1]\n",
        "\t# format all fields as string\n",
        "\tX = X.astype(str)\n",
        "\t# reshape target to be a 2d array\n",
        "\ty = y.reshape((len(y), 1))\n",
        "\treturn X, y\n",
        "\n",
        "# load the dataset\n",
        "X, y = load_dataset(PATH_2_DATA)\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1, shuffle=True)\n",
        "# summarize\n",
        "print('Train', X_train.shape, y_train.shape)\n",
        "print('Test', X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (3424, 10) (3424, 1)\n",
            "Test (1687, 10) (1687, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC1NUoEKT0Sy",
        "outputId": "7c159cd1-cb38-42ef-a3ec-cb4d68b1026a"
      },
      "source": [
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.33, random_state=1, shuffle=True)\n",
        "# summarize\n",
        "print('Train', X_train.shape, y_train.shape)\n",
        "print('Test', X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (3423, 17) (3423, 1)\n",
            "Test (1687, 17) (1687, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1VadYSQV16J"
      },
      "source": [
        "# Split into training and test set\n",
        "features_train, features_test, target_train, target_test = train_test_split(X.values, y, test_size=0.2, random_state=1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAWp1-sFjusf",
        "outputId": "bd3240c8-c6c0-48c9-f468-39058a1cf533"
      },
      "source": [
        "train.label.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1.0\n",
              "1    0.0\n",
              "2    1.0\n",
              "3    4.0\n",
              "4    0.0\n",
              "Name: label, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTKbscC3j2u9"
      },
      "source": [
        "# Seperate the features and labels\n",
        "targets_np = train.label.values\n",
        "features_np = train.loc[:, train.columns != 'label'].values/255\n",
        "\n",
        "# Split into training and test set\n",
        "features_train, features_test, target_train, target_test = train_test_split(features_np, targets_np, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVokFPY6XUEC",
        "outputId": "d5f183ee-fe3a-4643-d7c0-b91eff7cb00e"
      },
      "source": [
        "print(features_train.shape, features_test.shape, target_train.shape, target_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33600, 784) (8400, 784) (33600,) (8400,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX019uYwkCGd"
      },
      "source": [
        "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
        "featuresTrain = torch.from_numpy(features_train)\n",
        "targetsTrain = torch.from_numpy(target_train).type(torch.LongTensor) # data type is long\n",
        "\n",
        "# create feature and targets tensor for test set.\n",
        "featuresTest = torch.from_numpy(features_test)\n",
        "targetsTest = torch.from_numpy(target_test).type(torch.LongTensor) # data type is long"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6FgOoCFmPUA",
        "outputId": "54df7848-a8ae-454e-df25-21d61d5ce831"
      },
      "source": [
        "print(featuresTrain.shape, targetsTrain.shape, featuresTest.shape, targetsTest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([33600, 784]) torch.Size([33600]) torch.Size([8400, 784]) torch.Size([8400])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHiO_w1TkClo"
      },
      "source": [
        "# Set batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Pytorch train and test sets\n",
        "train_data = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
        "test_data = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
        "\n",
        "# data loader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-e6uUTVqYJ7"
      },
      "source": [
        "num_teachers = 10 # we're working with 100 teachers\n",
        "num_examples = len(train_data) // num_teachers # the size of each teacher's dataset\n",
        "num_workers = 0 # number of subprocesses for data loading\n",
        "batch_size = 64 # number of samples per batch\n",
        "\n",
        "# Split the data among all teachers\n",
        "teacher_loaders = []\n",
        "\n",
        "for i in range(num_teachers):\n",
        "    indices = list(range(i * num_examples, (i+1) * num_examples))\n",
        "    data = Subset(train_data, indices)\n",
        "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, num_workers=num_workers)\n",
        "    teacher_loaders.append(loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "dhsyLwp_kElO",
        "outputId": "a7832f00-fcba-4d41-e511-42e9499d4be3"
      },
      "source": [
        "# visualize one of the images in data set\n",
        "def visualize_image(data, index, pred=False, val=0):\n",
        "    '''This funtion can be used to visualize the images'''\n",
        "    plt.imshow(data[index].reshape(28,28))\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Handwritten Digit Image\")\n",
        "    plt.show()\n",
        "visualize_image(features_np, 12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYaZ1_GUkGCa",
        "outputId": "a86b89a3-2d13-4f24-cb9d-1454108b8b36"
      },
      "source": [
        "featuresTrain.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4088, 17])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNXCbEEikKTX"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 5 Hidden Layer Network\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "        \n",
        "        # Dropout module with 0.2 probbability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        # Add softmax on output layer\n",
        "        self.log_softmax = F.log_softmax\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "        x = self.dropout(F.relu(self.fc4(x)))\n",
        "        \n",
        "        x = self.log_softmax(self.fc5(x), dim=1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgwXkOtgg8uF"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 5 Hidden Layer Network\n",
        "        self.fc1 = nn.Linear(17, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 2)\n",
        "        \n",
        "        # Dropout module with 0.2 probbability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        # Add softmax on output layer\n",
        "        self.log_softmax = F.log_softmax\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "        x = self.dropout(F.relu(self.fc4(x)))\n",
        "        \n",
        "        x = self.log_softmax(self.fc5(x))\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c39dmHB7kNRe"
      },
      "source": [
        "model = Classifier()#.double()\n",
        "# Define our loss function\n",
        "criterion = nn.NLLLoss()\n",
        "# Define the optimier\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0015)\n",
        "\n",
        "epochs = 5\n",
        "steps = 0\n",
        "print_every = 5\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        # Prevent accumulation of gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Make predictions\n",
        "        log_ps = model(images)\n",
        "        loss = criterion(log_ps, labels)\n",
        "        #backprop\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "\n",
        "            # Turn off gradients for validation\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    log_ps = model(images)\n",
        "                    test_loss += criterion(log_ps, labels)\n",
        "\n",
        "                    ps = torch.exp(log_ps)\n",
        "                    # Get our top predictions\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_losses.append(running_loss/len(train_loader))\n",
        "            test_losses.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "                  \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjSLSJvIlBQb"
      },
      "source": [
        "def train(model, trainloader, criterion, optimizer, epochs=10):\n",
        "  train_losses, test_losses = [], []\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        \n",
        "        # Prevent accumulation of gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Make predictions\n",
        "        log_ps = model(images)\n",
        "        \n",
        "        loss = criterion(log_ps, labels)\n",
        "        #backprop\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    print(\"Epoc: \", e+1,  \"Loss: \", running_loss)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ-BNHGjiCv0"
      },
      "source": [
        "def train(model, trainloader, criterion, optimizer, epochs=10):\n",
        "  train_losses, test_losses = [], []\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        \n",
        "        # Prevent accumulation of gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Make predictions\n",
        "        log_ps = model(images)\n",
        "        loss = criterion(log_ps, labels)\n",
        "        #backprop\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    print(\"Epoc: \", e+1,  \"Loss: \", running_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65vPVY2LtxZK"
      },
      "source": [
        "**MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U0zCFaWlWJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb724c9-720e-4324-f872-e4aedfa5ffee"
      },
      "source": [
        "teacher_models = []\n",
        "num_teachers = 10\n",
        "for i in range(num_teachers):\n",
        "    print(\"Training Teacher #\", i+1)\n",
        "    model = Classifier()\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    train(model, train_loader, criterion, optimizer)\n",
        "    teacher_models.append(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Teacher # 1\n",
            "Epoc:  1 Loss:  257.078210901469\n",
            "Epoc:  2 Loss:  94.46797699481249\n",
            "Epoc:  3 Loss:  68.48429171741009\n",
            "Epoc:  4 Loss:  56.588981638196856\n",
            "Epoc:  5 Loss:  43.93288158951327\n",
            "Epoc:  6 Loss:  40.60950640810188\n",
            "Epoc:  7 Loss:  32.43119333893992\n",
            "Epoc:  8 Loss:  30.201596719911322\n",
            "Epoc:  9 Loss:  27.732783639919944\n",
            "Epoc:  10 Loss:  21.782293470576406\n",
            "Training Teacher # 2\n",
            "Epoc:  1 Loss:  250.89323365315795\n",
            "Epoc:  2 Loss:  90.90833532810211\n",
            "Epoc:  3 Loss:  67.03861889708787\n",
            "Epoc:  4 Loss:  51.38085844414309\n",
            "Epoc:  5 Loss:  45.09451748826541\n",
            "Epoc:  6 Loss:  38.44927969807759\n",
            "Epoc:  7 Loss:  32.75459095090628\n",
            "Epoc:  8 Loss:  28.616444741142914\n",
            "Epoc:  9 Loss:  26.923179618897848\n",
            "Epoc:  10 Loss:  25.008857161796186\n",
            "Training Teacher # 3\n",
            "Epoc:  1 Loss:  256.8853597790003\n",
            "Epoc:  2 Loss:  92.26163549907506\n",
            "Epoc:  3 Loss:  68.42948478460312\n",
            "Epoc:  4 Loss:  53.268639070447534\n",
            "Epoc:  5 Loss:  45.48750689555891\n",
            "Epoc:  6 Loss:  40.25153160607442\n",
            "Epoc:  7 Loss:  32.946156069636345\n",
            "Epoc:  8 Loss:  29.868824167293496\n",
            "Epoc:  9 Loss:  24.93581773468759\n",
            "Epoc:  10 Loss:  25.513416887253697\n",
            "Training Teacher # 4\n",
            "Epoc:  1 Loss:  260.7768019475043\n",
            "Epoc:  2 Loss:  94.86670542322099\n",
            "Epoc:  3 Loss:  68.87443095911294\n",
            "Epoc:  4 Loss:  53.98091297224164\n",
            "Epoc:  5 Loss:  43.916983420494944\n",
            "Epoc:  6 Loss:  37.93450295459479\n",
            "Epoc:  7 Loss:  34.1480310310144\n",
            "Epoc:  8 Loss:  28.53478435089346\n",
            "Epoc:  9 Loss:  28.097312420024537\n",
            "Epoc:  10 Loss:  25.34083387942519\n",
            "Training Teacher # 5\n",
            "Epoc:  1 Loss:  255.6653152704239\n",
            "Epoc:  2 Loss:  93.5999140497297\n",
            "Epoc:  3 Loss:  68.26227901037782\n",
            "Epoc:  4 Loss:  54.49206354073249\n",
            "Epoc:  5 Loss:  47.129361724015325\n",
            "Epoc:  6 Loss:  38.001466650515795\n",
            "Epoc:  7 Loss:  33.07513292791555\n",
            "Epoc:  8 Loss:  30.302166600245982\n",
            "Epoc:  9 Loss:  27.85977920150617\n",
            "Epoc:  10 Loss:  23.557978851371445\n",
            "Training Teacher # 6\n",
            "Epoc:  1 Loss:  251.36467857286334\n",
            "Epoc:  2 Loss:  92.45516868494451\n",
            "Epoc:  3 Loss:  70.52356870751828\n",
            "Epoc:  4 Loss:  55.71478751627728\n",
            "Epoc:  5 Loss:  44.63592273974791\n",
            "Epoc:  6 Loss:  36.90969038172625\n",
            "Epoc:  7 Loss:  35.20357567293104\n",
            "Epoc:  8 Loss:  30.49326176859904\n",
            "Epoc:  9 Loss:  26.709668767405674\n",
            "Epoc:  10 Loss:  24.97343493171502\n",
            "Training Teacher # 7\n",
            "Epoc:  1 Loss:  267.0931014306843\n",
            "Epoc:  2 Loss:  96.24420845508575\n",
            "Epoc:  3 Loss:  68.67615594435483\n",
            "Epoc:  4 Loss:  56.337970518739894\n",
            "Epoc:  5 Loss:  45.95970900240354\n",
            "Epoc:  6 Loss:  38.35426909546368\n",
            "Epoc:  7 Loss:  35.17603076878004\n",
            "Epoc:  8 Loss:  31.741827514371835\n",
            "Epoc:  9 Loss:  27.897334346547723\n",
            "Epoc:  10 Loss:  24.687418381974567\n",
            "Training Teacher # 8\n",
            "Epoc:  1 Loss:  254.93963995575905\n",
            "Epoc:  2 Loss:  94.08730583451688\n",
            "Epoc:  3 Loss:  69.76231518248096\n",
            "Epoc:  4 Loss:  52.913910957053304\n",
            "Epoc:  5 Loss:  47.02142602857202\n",
            "Epoc:  6 Loss:  39.127689176704735\n",
            "Epoc:  7 Loss:  32.46260053408332\n",
            "Epoc:  8 Loss:  30.008219725394156\n",
            "Epoc:  9 Loss:  27.01680298463907\n",
            "Epoc:  10 Loss:  24.365424027724657\n",
            "Training Teacher # 9\n",
            "Epoc:  1 Loss:  250.54162042960525\n",
            "Epoc:  2 Loss:  93.07477975264192\n",
            "Epoc:  3 Loss:  67.53714609146118\n",
            "Epoc:  4 Loss:  51.28788567474112\n",
            "Epoc:  5 Loss:  44.7549272084143\n",
            "Epoc:  6 Loss:  38.6522034292575\n",
            "Epoc:  7 Loss:  32.78687750373501\n",
            "Epoc:  8 Loss:  27.844817613135092\n",
            "Epoc:  9 Loss:  27.400910515803844\n",
            "Epoc:  10 Loss:  24.980106636765413\n",
            "Training Teacher # 10\n",
            "Epoc:  1 Loss:  259.88561545684934\n",
            "Epoc:  2 Loss:  92.27960828319192\n",
            "Epoc:  3 Loss:  66.71988359419629\n",
            "Epoc:  4 Loss:  53.192118800012395\n",
            "Epoc:  5 Loss:  46.622499585850164\n",
            "Epoc:  6 Loss:  38.8490738843102\n",
            "Epoc:  7 Loss:  35.014939433080144\n",
            "Epoc:  8 Loss:  30.99400504701771\n",
            "Epoc:  9 Loss:  25.576927235466428\n",
            "Epoc:  10 Loss:  24.003885393613018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pztj2Zoqt1NX"
      },
      "source": [
        "**Medical**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YtijgpyrX3W",
        "outputId": "41b753bc-be09-4f81-d345-72ef24dbc815"
      },
      "source": [
        "num_teachers = 10\n",
        "teacher_models = []\n",
        "for i in range(num_teachers):\n",
        "    print(\"Training Teacher #\", i+1)\n",
        "    model = Classifier().double()\n",
        "    criterion = nn.NLLLoss()\n",
        "    #criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    train(model,  teacher_loaders[i], criterion, optimizer)\n",
        "    teacher_models.append(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Teacher # 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoc:  1 Loss:  1490.4409599694832\n",
            "Epoc:  2 Loss:  166.3463599720209\n",
            "Epoc:  3 Loss:  45.389025063097264\n",
            "Epoc:  4 Loss:  30.46285791977291\n",
            "Epoc:  5 Loss:  24.779855210268476\n",
            "Epoc:  6 Loss:  22.10638364611048\n",
            "Epoc:  7 Loss:  21.90820649206878\n",
            "Epoc:  8 Loss:  18.039951453998665\n",
            "Epoc:  9 Loss:  16.853178046953893\n",
            "Epoc:  10 Loss:  16.705453956129332\n",
            "Training Teacher # 2\n",
            "Epoc:  1 Loss:  1158.3635490921533\n",
            "Epoc:  2 Loss:  113.86759564449139\n",
            "Epoc:  3 Loss:  43.644518071118156\n",
            "Epoc:  4 Loss:  25.384072162610465\n",
            "Epoc:  5 Loss:  26.190761036274814\n",
            "Epoc:  6 Loss:  20.187878430796772\n",
            "Epoc:  7 Loss:  18.446984600670636\n",
            "Epoc:  8 Loss:  21.23191183364489\n",
            "Epoc:  9 Loss:  16.887795552605574\n",
            "Epoc:  10 Loss:  16.14658159696143\n",
            "Training Teacher # 3\n",
            "Epoc:  1 Loss:  734.6863362881437\n",
            "Epoc:  2 Loss:  92.01154995821322\n",
            "Epoc:  3 Loss:  30.1133383292286\n",
            "Epoc:  4 Loss:  25.141600766117136\n",
            "Epoc:  5 Loss:  23.388668865254914\n",
            "Epoc:  6 Loss:  19.48378447112353\n",
            "Epoc:  7 Loss:  17.985220113454776\n",
            "Epoc:  8 Loss:  18.880488815613916\n",
            "Epoc:  9 Loss:  18.235641868357607\n",
            "Epoc:  10 Loss:  17.100401037889146\n",
            "Training Teacher # 4\n",
            "Epoc:  1 Loss:  817.0022574062415\n",
            "Epoc:  2 Loss:  108.39287611757852\n",
            "Epoc:  3 Loss:  52.372622847064896\n",
            "Epoc:  4 Loss:  25.602927581217585\n",
            "Epoc:  5 Loss:  24.886741465836806\n",
            "Epoc:  6 Loss:  18.71496773185359\n",
            "Epoc:  7 Loss:  19.01612149073475\n",
            "Epoc:  8 Loss:  17.15726730370518\n",
            "Epoc:  9 Loss:  17.268458355265278\n",
            "Epoc:  10 Loss:  16.92751758761469\n",
            "Training Teacher # 5\n",
            "Epoc:  1 Loss:  1162.5498045014147\n",
            "Epoc:  2 Loss:  144.90574130455886\n",
            "Epoc:  3 Loss:  51.211037648481934\n",
            "Epoc:  4 Loss:  37.158666427367365\n",
            "Epoc:  5 Loss:  24.33913730758535\n",
            "Epoc:  6 Loss:  20.97421830815663\n",
            "Epoc:  7 Loss:  25.025505847631035\n",
            "Epoc:  8 Loss:  17.448291572979553\n",
            "Epoc:  9 Loss:  18.693976299722433\n",
            "Epoc:  10 Loss:  15.292023295923686\n",
            "Training Teacher # 6\n",
            "Epoc:  1 Loss:  1425.8986726363019\n",
            "Epoc:  2 Loss:  153.58662048811686\n",
            "Epoc:  3 Loss:  63.54798714387209\n",
            "Epoc:  4 Loss:  38.72241210558087\n",
            "Epoc:  5 Loss:  26.103597511111996\n",
            "Epoc:  6 Loss:  27.3364945588144\n",
            "Epoc:  7 Loss:  23.50599920333676\n",
            "Epoc:  8 Loss:  18.907596819709713\n",
            "Epoc:  9 Loss:  18.954511123172395\n",
            "Epoc:  10 Loss:  19.409040220970144\n",
            "Training Teacher # 7\n",
            "Epoc:  1 Loss:  905.042289255265\n",
            "Epoc:  2 Loss:  119.71009475088127\n",
            "Epoc:  3 Loss:  54.2952892119858\n",
            "Epoc:  4 Loss:  39.52937863110485\n",
            "Epoc:  5 Loss:  32.01561154167648\n",
            "Epoc:  6 Loss:  24.579199855901\n",
            "Epoc:  7 Loss:  18.64333349457489\n",
            "Epoc:  8 Loss:  17.600389974916805\n",
            "Epoc:  9 Loss:  16.023908200499108\n",
            "Epoc:  10 Loss:  15.00344657252508\n",
            "Training Teacher # 8\n",
            "Epoc:  1 Loss:  1135.3910786481279\n",
            "Epoc:  2 Loss:  150.02218111965368\n",
            "Epoc:  3 Loss:  41.72231462472776\n",
            "Epoc:  4 Loss:  34.6264783551146\n",
            "Epoc:  5 Loss:  26.493503258688218\n",
            "Epoc:  6 Loss:  20.66009780411417\n",
            "Epoc:  7 Loss:  18.973535893836814\n",
            "Epoc:  8 Loss:  17.77367475177963\n",
            "Epoc:  9 Loss:  17.020120853007572\n",
            "Epoc:  10 Loss:  15.823052186051427\n",
            "Training Teacher # 9\n",
            "Epoc:  1 Loss:  1014.2435043108977\n",
            "Epoc:  2 Loss:  154.68530871770244\n",
            "Epoc:  3 Loss:  55.0398070271466\n",
            "Epoc:  4 Loss:  29.12414458626262\n",
            "Epoc:  5 Loss:  26.879994029289843\n",
            "Epoc:  6 Loss:  20.709544896259533\n",
            "Epoc:  7 Loss:  19.16648891531545\n",
            "Epoc:  8 Loss:  18.468133479209538\n",
            "Epoc:  9 Loss:  18.60464886797337\n",
            "Epoc:  10 Loss:  14.615394591744112\n",
            "Training Teacher # 10\n",
            "Epoc:  1 Loss:  1157.1157415172627\n",
            "Epoc:  2 Loss:  214.99311124123466\n",
            "Epoc:  3 Loss:  80.4690884231155\n",
            "Epoc:  4 Loss:  43.83623869216246\n",
            "Epoc:  5 Loss:  33.508052784035556\n",
            "Epoc:  6 Loss:  24.355524414780334\n",
            "Epoc:  7 Loss:  20.867041590843257\n",
            "Epoc:  8 Loss:  19.540622423489463\n",
            "Epoc:  9 Loss:  17.154175774011335\n",
            "Epoc:  10 Loss:  17.201252296218527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qbnA1b12jCQ"
      },
      "source": [
        "batch_size = 64\n",
        "student_train_size = int(len(test_data) * 0.05) # 5% of data for training - 500 samples\n",
        "student_test_size = int(len(test_data) * 0.05)  # 5% of data for testing - 500 samples\n",
        "\n",
        "student_train_data = Subset(test_data, list(range(student_train_size)))\n",
        "student_test_data = Subset(test_data, list(range(student_train_size, student_train_size + student_test_size)))\n",
        "\n",
        "student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "student_test_loader = torch.utils.data.DataLoader(student_test_data, batch_size=batch_size, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R75u-nSqpPJz"
      },
      "source": [
        "batch_size = 64\n",
        "student_train_size = int(len(test_data)*0.5) # 5% of data for training - 500 samples\n",
        "student_test_size = int(len(test_data)*0.5)  # 5% of data for testing - 500 samples\n",
        "\n",
        "student_train_data = Subset(test_data, list(range(student_train_size)))\n",
        "student_test_data = Subset(test_data, list(range(student_train_size, student_train_size + student_test_size)))\n",
        "\n",
        "student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "student_test_loader = torch.utils.data.DataLoader(student_test_data, batch_size=batch_size, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaZCgDMcsCrQ"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def predict(model, dataloader):\n",
        "    outputs = torch.zeros(0, dtype=torch.long).to(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model.forward(images)\n",
        "        ps = torch.argmax(torch.exp(output), dim=1)\n",
        "        outputs = torch.cat((outputs, ps))\n",
        "    \n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLpBrlRzt_Dm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8W1VgDZt_Y-"
      },
      "source": [
        "**Medical**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbI8Wdm9sIBI"
      },
      "source": [
        "epsilon = 0.1 (10)\n",
        "#Data Independent Epsilon: 168.63292546497027\n",
        "#Data Dependent Epsilon: 168.6329254649676\n",
        "\n",
        "Epsilon:  0.1 (20)\n",
        "#Data Independent Epsilon: 31.15292546497023\n",
        "#Data Dependent Epsilon: 31.15292546496999\n",
        "\n",
        "\n",
        "\n",
        "epsilon = 0.2\n",
        "#Data Independent Epsilon: 90.07292546497024\n",
        "#Data Dependent Epsilon: 90.07292546496926\n",
        "\n",
        "epsilon = 0.15\n",
        "#Data Independent Epsilon: 55.70292546497023\n",
        "#Data Dependent Epsilon: 55.70292546497065\n",
        "\n",
        "epsilon = 0.15\n",
        "#Data Independent Epsilon: 55.70292546497023\n",
        "#Data Dependent Epsilon: 55.70292546497065\n",
        "\n",
        "Data Independent Epsilon: 2.231492546497023\n",
        "Data Dependent Epsilon: 2.2314925464970288\n",
        "Epsilon:  0.02\n",
        "Data Independent Epsilon: 4.659385092994046\n",
        "Data Dependent Epsilon: 4.65938509299402\n",
        "Epsilon:  0.03\n",
        "Data Independent Epsilon: 7.297231366242556\n",
        "Data Dependent Epsilon: 7.29723136624254\n",
        "Epsilon:  0.04\n",
        "Data Independent Epsilon: 10.122441821656743\n",
        "Data Dependent Epsilon: 10.122441821656691\n",
        "\n",
        "Epsilon:  0.11\n",
        "Data Independent Epsilon: 35.277325464970225\n",
        "Data Dependent Epsilon: 35.27732546497052\n",
        "Epsilon:  0.12\n",
        "Data Independent Epsilon: 39.79452546497023\n",
        "Data Dependent Epsilon: 39.794525464970484\n",
        "Epsilon:  0.13\n",
        "Data Independent Epsilon: 44.70452546497023\n",
        "Data Dependent Epsilon: 44.70452546496997\n",
        "Epsilon:  0.13999999999999999\n",
        "Data Independent Epsilon: 50.00732546497022\n",
        "Data Dependent Epsilon: 50.007325464970116\n",
        "Epsilon:  0.14999999999999997\n",
        "Data Independent Epsilon: 55.70292546497021\n",
        "Data Dependent Epsilon: 55.70292546497065\n",
        "Epsilon:  0.15999999999999998\n",
        "Data Independent Epsilon: 61.791325464970214\n",
        "Data Dependent Epsilon: 61.79132546497062\n",
        "Epsilon:  0.16999999999999998\n",
        "Data Independent Epsilon: 68.27252546497022\n",
        "Data Dependent Epsilon: 68.27252546497047\n",
        "Epsilon:  0.17999999999999997\n",
        "Data Independent Epsilon: 75.1465254649702\n",
        "Data Dependent Epsilon: 75.14652546496933"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-ffgNgp0Ds-",
        "outputId": "86b86cda-80e4-4c2a-c566-2b1e4ee063fb"
      },
      "source": [
        "from numpy import arange\n",
        "ep = arange(0.01, 0.09, 0.01)\n",
        "ep"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 349
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V66Ca2wtBnSg"
      },
      "source": [
        "saved_teacher_models = teacher_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3kePTq9Bp8Y",
        "outputId": "8acd1b41-6065-4407-c876-42fc09ac9b17"
      },
      "source": [
        "print(len(teacher_models))\n",
        "teacher_models += teacher_models\n",
        "teacher_models += teacher_models\n",
        "teacher_models += teacher_models\n",
        "print(len(teacher_models))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO6l17XY0O-H"
      },
      "source": [
        "def PATE(epsilon=0.1):\n",
        "  preds = torch.torch.zeros((len(teacher_models[:20]), student_train_size), dtype=torch.long)\n",
        "\n",
        "  for i, model in enumerate(teacher_models[:20]):\n",
        "      results = predict(model, student_train_loader)\n",
        "      preds[i] = results\n",
        "\n",
        "  labels = np.array([]).astype(int)\n",
        "  for image_preds in np.transpose(preds):\n",
        "      label_counts = np.bincount(image_preds, minlength=10)\n",
        "      beta = 1 / epsilon\n",
        "\n",
        "      for i in range(len(label_counts)):\n",
        "          label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "      new_label = np.argmax(label_counts)\n",
        "      labels = np.append(labels, new_label)\n",
        "\n",
        "  PATE_labels = labels # Aggregate Teacher Labels   \n",
        "  #true_labels = test_data.targets[:student_train_size] # True Labels\n",
        "  PATE_preds = preds # Labels Obtained by Teachers\n",
        "\n",
        "  print(\"Epsilon: \", epsilon)\n",
        "\n",
        "  data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=PATE_preds, \n",
        "                                                   indices=PATE_labels, \n",
        "                                                   noise_eps=epsilon, \n",
        "                                                   delta=1e-5, \n",
        "                                                   moments=10)\n",
        "  \n",
        "  print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "  print(\"Data Dependent Epsilon:\", data_dep_eps)\n",
        "  return data_ind_eps, data_dep_eps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qDvdDVRB5NY",
        "outputId": "458e3a5c-7d78-433e-f026-830d6264d958"
      },
      "source": [
        "PATE(epsilon=0.11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epsilon:  0.11\n",
            "Data Independent Epsilon: 31.840925464970226\n",
            "Data Dependent Epsilon: 31.840925464970454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31.840925464970226, 31.840925464970454)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31o3pK3pvxIu"
      },
      "source": [
        "Epsilon:  0.11 (1)\n",
        "Data Independent Epsilon: 31.840925464970226\n",
        "Data Dependent Epsilon: 31.840925464970454\n",
        "Epoch: 50/50..  Training Loss: 0.006..  Test Loss: 0.700..  Test Accuracy: 0.866\n",
        "\n",
        "Epsilon:  0.11 (10)\n",
        "Data Independent Epsilon: 31.840925464970226\n",
        "Data Dependent Epsilon: 31.840925464970454\n",
        "Epoch: 47/50..  Training Loss: 0.014..  Test Loss: 0.689..  Test Accuracy: 0.876"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUX-wYo60ghZ"
      },
      "source": [
        "data_ind_eps = []\n",
        "data_dep_eps = []\n",
        "\n",
        "for e in ep:\n",
        "  data_ind_ep, data_dep_ep = PATE(epsilon=e)\n",
        "  data_ind_eps.append(data_ind_ep)\n",
        "  data_dep_eps.append(data_dep_ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0avRRc0WsFJe",
        "outputId": "7cd42706-7dcd-46af-bcfe-c471f749072b"
      },
      "source": [
        "preds = torch.torch.zeros((len(teacher_models), student_train_size), dtype=torch.long)\n",
        "\n",
        "for i, model in enumerate(teacher_models):\n",
        "    results = predict(model, student_train_loader)\n",
        "    preds[i] = results\n",
        "\n",
        "labels = np.array([]).astype(int)\n",
        "for image_preds in np.transpose(preds):\n",
        "    label_counts = np.bincount(image_preds, minlength=10)\n",
        "    beta = 1 / epsilon\n",
        "\n",
        "    for i in range(len(label_counts)):\n",
        "        label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "    new_label = np.argmax(label_counts)\n",
        "    labels = np.append(labels, new_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgxT5r1G283f"
      },
      "source": [
        "PATE_labels = labels # Aggregate Teacher Labels   \n",
        "#true_labels = test_data.targets[:student_train_size] # True Labels\n",
        "PATE_preds = preds # Labels Obtained by Teachers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCrg-VW3OZo",
        "outputId": "1f8fa3b6-e115-40ba-cc97-f617c3d42ac9"
      },
      "source": [
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=PATE_preds, \n",
        "                                                   indices=PATE_labels, \n",
        "                                                   noise_eps=epsilon, \n",
        "                                                   delta=1e-5, \n",
        "                                                   moments=10)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Independent Epsilon: 55.70292546497023\n",
            "Data Dependent Epsilon: 55.70292546497065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4-pbv153bpv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd8vEJm_9iJ1"
      },
      "source": [
        "student_train_data = Subset(test_data, list(range(student_train_size)))\n",
        "student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC5kSAIr981t",
        "outputId": "d7b84411-d258-46b4-bdd7-a9d89f890e03"
      },
      "source": [
        "def train_student(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
        "    \n",
        "    model.to(device)\n",
        "    running_loss = 0\n",
        "    steps = 0\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            steps += 1\n",
        "            \n",
        "            # 1) erase previous gradients (if they exist)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 2) make a prediction\n",
        "            pred = model.forward(images)\n",
        "\n",
        "            # 3) calculate how much we missed\n",
        "            loss = criterion(pred, labels)\n",
        "\n",
        "            # 4) figure out which weights caused us to miss\n",
        "            loss.backward()\n",
        "\n",
        "            # 5) change those weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # 6) log our progress\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            if steps % 25 == 0:\n",
        "                test_loss = 0\n",
        "                accuracy = 0\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for images, labels in test_loader:\n",
        "                        images, labels = images.to(device), labels.to(device)\n",
        "                        log_ps = model(images)\n",
        "                        test_loss += criterion(log_ps, labels).item()\n",
        "\n",
        "                        # Accuracy\n",
        "                        ps = torch.exp(log_ps)\n",
        "                        top_p, top_class = ps.topk(1, dim=1)\n",
        "                        equals = top_class == labels.view(*top_class.shape)\n",
        "                        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "                model.train()\n",
        "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                      \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
        "                      \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
        "                      \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
        "                running_loss = 0\n",
        "             \n",
        "            \n",
        "\n",
        "student_model = Classifier()#.double()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.002)\n",
        "epochs = 50\n",
        "train_student(student_model, student_train_loader, student_test_loader, criterion, optimizer, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4/50..  Training Loss: 6.044..  Test Loss: 0.881..  Test Accuracy: 0.685\n",
            "Epoch: 8/50..  Training Loss: 2.329..  Test Loss: 0.571..  Test Accuracy: 0.797\n",
            "Epoch: 11/50..  Training Loss: 1.051..  Test Loss: 0.577..  Test Accuracy: 0.831\n",
            "Epoch: 15/50..  Training Loss: 0.399..  Test Loss: 0.564..  Test Accuracy: 0.852\n",
            "Epoch: 18/50..  Training Loss: 0.274..  Test Loss: 0.559..  Test Accuracy: 0.844\n",
            "Epoch: 22/50..  Training Loss: 0.129..  Test Loss: 0.574..  Test Accuracy: 0.863\n",
            "Epoch: 25/50..  Training Loss: 0.107..  Test Loss: 0.718..  Test Accuracy: 0.825\n",
            "Epoch: 29/50..  Training Loss: 0.098..  Test Loss: 0.735..  Test Accuracy: 0.854\n",
            "Epoch: 33/50..  Training Loss: 0.269..  Test Loss: 0.541..  Test Accuracy: 0.863\n",
            "Epoch: 36/50..  Training Loss: 0.085..  Test Loss: 0.644..  Test Accuracy: 0.856\n",
            "Epoch: 40/50..  Training Loss: 0.041..  Test Loss: 0.733..  Test Accuracy: 0.863\n",
            "Epoch: 43/50..  Training Loss: 0.077..  Test Loss: 0.523..  Test Accuracy: 0.886\n",
            "Epoch: 47/50..  Training Loss: 0.051..  Test Loss: 0.674..  Test Accuracy: 0.860\n",
            "Epoch: 50/50..  Training Loss: 0.011..  Test Loss: 0.687..  Test Accuracy: 0.871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_V7aQPM-Hja",
        "outputId": "d885b227-8dcc-4e5e-9047-db3d69e6d5ee"
      },
      "source": [
        "test_loss = 0\n",
        "accuracy = 0\n",
        "student_model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in student_test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        log_ps = student_model(images)\n",
        "        test_loss += criterion(log_ps, labels).item()\n",
        "\n",
        "        # Accuracy\n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "student_model.train()\n",
        "print(\"Test Loss: {:.3f}.. \".format(test_loss/len(student_test_loader)),\n",
        "      \"Test Accuracy: {:.3f}\".format(accuracy/len(student_test_loader)))\n",
        "running_loss = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.938..  Test Accuracy: 0.837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kIl6WTwluPVO",
        "outputId": "6c91a860-b6f1-49ee-8e8b-52d439d6189f"
      },
      "source": [
        "!pip install syft==0.2.9\n",
        "from syft.frameworks.torch.dp import pate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft==0.2.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/73/891ba1dca7e0ba77be211c36688f083184d8c9d5901b8cd59cbf867052f3/syft-0.2.9-py3-none-any.whl (433kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 9.0MB/s \n",
            "\u001b[?25hCollecting torch~=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n",
            "\u001b[?25hCollecting psutil==5.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/b8/3512f0e93e0db23a71d82485ba256071ebef99b227351f0f5540f744af41/psutil-5.7.0.tar.gz (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 34.8MB/s \n",
            "\u001b[?25hCollecting openmined.threepio==0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/38/df6367693c7f3808f076cd8c2647c434a04adda2bbb2435dadefe7258fd4/openmined.threepio-0.2.0.tar.gz (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.5MB/s \n",
            "\u001b[?25hCollecting requests-toolbelt==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
            "\u001b[?25hCollecting RestrictedPython~=5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/cc/28c4d966615a46b03be4dac0f2c6e713412efbf2f85428eeb9618c4f6f0c/RestrictedPython-5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dill~=0.3.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.3.3)\n",
            "Collecting notebook==5.7.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/36/89ebfffc9dd8c8dbd81c1ffb53e3d4233ee666414c143959477cb07cc5f5/notebook-5.7.8-py2.py3-none-any.whl (9.0MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.0.2)\n",
            "Collecting requests~=2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hCollecting numpy~=1.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.5MB/s \n",
            "\u001b[?25hCollecting flask-socketio~=4.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/66/44/edc4715af85671b943c18ac8345d0207972284a0cd630126ff5251faa08b/Flask_SocketIO-4.2.1-py2.py3-none-any.whl\n",
            "Collecting phe~=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/32/0e/568e97b014eb14e794a1258a341361e9da351dc6240c63b89e1541e3341c/phe-1.4.0.tar.gz\n",
            "Requirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.1.2)\n",
            "Requirement already satisfied: Pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (7.1.2)\n",
            "Collecting websocket-client~=0.57.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 42.0MB/s \n",
            "\u001b[?25hCollecting tblib~=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/de/dca3e651ca62e59c08d324f4a51467fa4b8cbeaafb883b5e83720b4d4a47/tblib-1.6.0-py2.py3-none-any.whl\n",
            "Collecting tornado==4.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/7b/e29ab3d51c8df66922fea216e2bddfcb6430fb29620e5165b16a216e0d3c/tornado-4.5.3.tar.gz (484kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 36.0MB/s \n",
            "\u001b[?25hCollecting shaloop==0.2.1-alpha.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/8e/6c4493280d55199161c2eea896327c740195cf16cc74c5393c08eababc83/shaloop-0.2.1_alpha.11-py3-none-manylinux1_x86_64.whl (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 41.5MB/s \n",
            "\u001b[?25hCollecting lz4~=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/38/dacc3cbb33a9ded9e2e57f48707e8842f1080997901578ebddaa0e031646/lz4-3.0.2-cp37-cp37m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.4.1)\n",
            "Collecting aiortc==0.9.28\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/c5/0c15e562c5ea1531e8c7db1bcd524e53619cc27a228f3f28d2ba55544d38/aiortc-0.9.28-cp37-cp37m-manylinux2010_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 38.7MB/s \n",
            "\u001b[?25hCollecting torchvision~=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/32/cb0e4c43cd717da50258887b088471568990b5a749784c465a8a1962e021/torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 35.0MB/s \n",
            "\u001b[?25hCollecting websockets~=8.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25hCollecting syft-proto~=0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/88/67edf7836ac4eab723416933cd663c4f87753d3ff31337f91701c0b75474/syft_proto-0.5.3-py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n",
            "\u001b[?25hCollecting importlib-resources~=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/2d/88f166bcaadc09d9fdbf1c336ad118e01b7fe1155e15675e125be2ff1899/importlib_resources-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.7.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.10.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (22.0.3)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.0.5)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.9.4)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (1.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (2.11.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.6.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.10.1)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.3.5)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.1.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.2.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (2020.12.5)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (1.24.3)\n",
            "Collecting python-socketio>=4.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/87/4503777f71dc2b125b18c43989d0afdf0e7306d0d8682ec6691209cc9329/python_socketio-5.2.1-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from websocket-client~=0.57.0->syft==0.2.9) (1.15.0)\n",
            "Requirement already satisfied: pycparser>=2 in /usr/local/lib/python3.7/dist-packages (from shaloop==0.2.1-alpha.11->syft==0.2.9) (2.20)\n",
            "Requirement already satisfied: cffi>=1 in /usr/local/lib/python3.7/dist-packages (from shaloop==0.2.1-alpha.11->syft==0.2.9) (1.14.5)\n",
            "Collecting aioice<0.7.0,>=0.6.17\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/86/e3cdf660b67da7a9a7013253db5db7cf786a52296cb40078db1206177698/aioice-0.6.18-py3-none-any.whl\n",
            "Collecting pylibsrtp>=0.5.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/24/9c2060d5f2d831091c7bb41428d17fd20e839959f1e78e6930329b21c0a7/pylibsrtp-0.6.8-cp37-cp37m-manylinux2010_x86_64.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.5MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 36.2MB/s \n",
            "\u001b[?25hCollecting av<9.0.0,>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/ff/bacde7314c646a2bd2f240034809a10cc3f8b096751284d0828640fff3dd/av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2MB)\n",
            "\u001b[K     |████████████████████████████████| 37.2MB 76kB/s \n",
            "\u001b[?25hCollecting pyee>=6.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/0a/933b3931107e1da186963fd9bb9bceb9a613cff034cb0fb3b0c61003f357/pyee-8.1.0-py2.py3-none-any.whl\n",
            "Collecting crc32c\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/85/4656cc0ac33be2725d6de41eabfeb72f86c194fe26decf28162d72f9a642/crc32c-2.2-cp37-cp37m-manylinux2010_x86_64.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from syft-proto~=0.5.2->syft==0.2.9) (3.12.4)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (3.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook==5.7.8->syft==0.2.9) (5.5.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook==5.7.8->syft==0.2.9) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook==5.7.8->syft==0.2.9) (1.1.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (2.6.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (1.4.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (3.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook==5.7.8->syft==0.2.9) (2.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook==5.7.8->syft==0.2.9) (2.6.0)\n",
            "Collecting bidict>=0.21.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/d4/eaf9242722bf991e0955380dd6168020cb15a71cc0d3cc2373f4911b1f1d/bidict-0.21.2-py2.py3-none-any.whl\n",
            "Collecting python-engineio>=4.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/f6/9515c8357935d4fe58b526ab5b4857245948fa1b5f607cf63d4663cce5c2/python_engineio-4.1.0-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
            "\u001b[?25hCollecting netifaces\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/18/fd6e9c71a35b67a73160ec80a49da63d1eed2d2055054cc2995714949132/netifaces-0.10.9.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->syft-proto~=0.5.2->syft==0.2.9) (56.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->importlib-resources~=1.5.0->syft==0.2.9) (3.7.4.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.8.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (20.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.2.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (2.4.7)\n",
            "Building wheels for collected packages: psutil, openmined.threepio, phe, tornado, netifaces\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.0-cp37-cp37m-linux_x86_64.whl size=276446 sha256=b875e0e27fb30d65b2a42704776bdcf2995382277fd6c8b660bba5c37d026d0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/69/b4/3200b95828d1f0ddb3cb5699083717f4fdbd9b4223d0644c57\n",
            "  Building wheel for openmined.threepio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openmined.threepio: filename=openmined.threepio-0.2.0-cp37-none-any.whl size=80095 sha256=75579b9ae46a295858f9f579bfb10f0072f9d8c35a6b90f52cf5739910b63a90\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/a5/c5/7e67449f5d4d487e1d3a583ba51d27403b315b18ef2e48a13c\n",
            "  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phe: filename=phe-1.4.0-py2.py3-none-any.whl size=37362 sha256=e133a1d9ef860c041c522c2437716128ee268f0d02d2678573ae7bfa3d7d2744\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/dc/36/dcb6bf0f1b9907e7b710ace63e64d08e7022340909315fdea4\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-4.5.3-cp37-cp37m-linux_x86_64.whl size=434001 sha256=35900aa39fcd62a657e478a80ad4ae917859fada4cf899d29c6a5fb2ab38602c\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/bf/f4/b68fa69596986881b397b18ff2b9af5f8181233aadcc9f76fd\n",
            "  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for netifaces: filename=netifaces-0.10.9-cp37-cp37m-linux_x86_64.whl size=37423 sha256=394b8cd40815425355f654d444837a058c85fde4e95b3408cb7e0566608936dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/8f/f3/7054578f04c904f70757c5c85a6e2823baa69d42365526e93d\n",
            "Successfully built psutil openmined.threepio phe tornado netifaces\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement notebook~=5.3.0; python_version >= \"3.0\", but you'll have notebook 5.7.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: bokeh 2.3.1 has requirement tornado>=5.1, but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, psutil, openmined.threepio, idna, requests, requests-toolbelt, RestrictedPython, tornado, notebook, numpy, bidict, python-engineio, python-socketio, flask-socketio, phe, websocket-client, tblib, shaloop, lz4, netifaces, aioice, pylibsrtp, cryptography, av, pyee, crc32c, aiortc, torchvision, websockets, syft-proto, importlib-resources, syft\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Found existing installation: notebook 5.3.1\n",
            "    Uninstalling notebook-5.3.1:\n",
            "      Successfully uninstalled notebook-5.3.1\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: tblib 1.7.0\n",
            "    Uninstalling tblib-1.7.0:\n",
            "      Successfully uninstalled tblib-1.7.0\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "  Found existing installation: importlib-resources 5.1.2\n",
            "    Uninstalling importlib-resources-5.1.2:\n",
            "      Successfully uninstalled importlib-resources-5.1.2\n",
            "Successfully installed RestrictedPython-5.1 aioice-0.6.18 aiortc-0.9.28 av-8.0.3 bidict-0.21.2 crc32c-2.2 cryptography-3.4.7 flask-socketio-4.2.1 idna-2.8 importlib-resources-1.5.0 lz4-3.0.2 netifaces-0.10.9 notebook-5.7.8 numpy-1.18.5 openmined.threepio-0.2.0 phe-1.4.0 psutil-5.7.0 pyee-8.1.0 pylibsrtp-0.6.8 python-engineio-4.1.0 python-socketio-5.2.1 requests-2.22.0 requests-toolbelt-0.9.1 shaloop-0.2.1a11 syft-0.2.9 syft-proto-0.5.3 tblib-1.6.0 torch-1.4.0 torchvision-0.5.0 tornado-4.5.3 websocket-client-0.57.0 websockets-8.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "numpy",
                  "psutil",
                  "requests",
                  "tblib",
                  "torch",
                  "torchvision",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}